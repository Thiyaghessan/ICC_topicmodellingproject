{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.test.utils import common_corpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KUNARAC = \"Kunarac\"\n",
    "BRIMA = \"Brima\"\n",
    "BEMBA = \"Bemba\"\n",
    "VICTIMS = \"victims\"\n",
    "JUDGEMENTS = \"judgements\"\n",
    "WITNESSES = \"Witnesses\"\n",
    "DEF_ITERATIONS = 1000\n",
    "DEF_TOPICS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['MALLET_HOME'] = \"C:\\\\Users\\\\thiya\\\\OneDrive\\\\NUS\\\\Year 3\\\\Sem 2\\\\RA\\\\Martin Buber\\\\Mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MALLET = \"C:\\\\Users\\\\thiya\\\\OneDrive\\\\NUS\\\\Year 3\\\\Sem 2\\\\RA\\\\Martin Buber\\\\Mallet\\\\bin\\\\mallet\"\n",
    "PATH_TO_TXT_MODELS = \"C:\\\\Users\\\\thiya\\\\OneDrive\\\\NUS\\\\Year 3\\\\Sem 2\\\\RA\\\\Matin Buber\\\\Results\"\n",
    "DATA_PATH = \"C:\\\\Users\\\\thiya\\\\OneDrive\\\\NUS\\\\Year 3\\\\Sem 2\\\\RA\\\\Martin Buber\\\\Data\"\n",
    "PROCESSED_FILES_FOLDER = \"C:\\\\Users\\\\thiya\\\\OneDrive\\\\NUS\\\\Year 3\\\\Sem 2\\\\RA\\\\Martin Buber\\\\Processed Files\"\n",
    "STOP_WORDS_PATH = \"C:\\\\Stop Words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_WORDS = [\"was\", \"us\", \"as\", \"less\", \"has\", \"media\", \"Serbs\", \"muslims\", \"raped\", \"man\", \"commander\", \"worked\",\n",
    "                 \"men\", \"does\", \"gave\", \"happened\", \"commander\", \"heard\", \"judes\", \"fo\"]\n",
    "\n",
    "stop_words = [\"haynes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir_path):\n",
    "    file_names = [name for name in listdir(dir_path) if isfile(join(dir_path, name))]\n",
    "    all_files = []\n",
    "    for file_name in file_names:\n",
    "        print(file_name)\n",
    "        n_file = open(dir_path + \"\\\\\" + file_name, \"r\", errors='ignore').readlines()\n",
    "        all_files.append(n_file)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_enumeration(file_list):\n",
    "    new_files = []\n",
    "    for file in file_list:\n",
    "        if file is not None:\n",
    "            lines = []\n",
    "            i = 0\n",
    "            for i in range(len(file)):\n",
    "                line = file[i].lower()\n",
    "                # delete lines announcing blank pages\n",
    "                if line == \"Blank page inserted to ensure pagination corresponds between the French and\":\n",
    "                    line = file[i+2].lower()\n",
    "                if \"open session\" not in line:\n",
    "                    if not (\"ENG\" in line and \"CT\" in line):\n",
    "                        line = line.split(\" \")\n",
    "                        if line[0] != '\\n':\n",
    "                            if line[0].isdigit():\n",
    "                                line = line[1:]\n",
    "                    lines.append(\" \".join(line))\n",
    "            new_files.append(lines)\n",
    "    return new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_files(file_list):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = []\n",
    "    for file in file_list:\n",
    "        t_file = \"\"\n",
    "        for line in file:\n",
    "            t_file += line\n",
    "        t_file = tokenizer.tokenize(t_file)\n",
    "        t_file = [word.lower() for word in t_file]\n",
    "        tokenized.append(t_file)\n",
    "    return tokenized\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_NER(file_list):\n",
    "    names = {\"radomir\": \"kovac\", \"klanfa\" : \"kovac\",\n",
    "             \"zaga\" : \"kunarac\", \"dragoljub\":\"kunarac\", \"zoran\" : \"vukovic\",\n",
    "              \"gullit\": \"alex-tamba-brima\",\n",
    "             \"saj\": \"musa\"}\n",
    "    double_names = {\"rojoj\": \"pass\", \"sierra\": \"leone\", \"buk\": \"bijela\",\n",
    "                    \"cerova\": \"ravan\", \"lepa\": \"brena\", \"kp\": \"dom\",\n",
    "                    \"junior\": \"lion\"}\n",
    "    triple_names = {\"santigie\": [\"borbor\", \"kanu\"]}\n",
    "    window = 3\n",
    "    new_files = []\n",
    "    i = 0\n",
    "    for file in file_list:\n",
    "        new_file = []\n",
    "        while i < len(file):\n",
    "            word = file[i].lower()\n",
    "            word_win = file[i - window:i + window]\n",
    "            word_win = [w.lower() for w in word_win]\n",
    "            if word == \"leone\":\n",
    "                    new_file.append(\"sierra-leone\")\n",
    "            elif word in names:\n",
    "                new_file.append(names[word])\n",
    "            elif word in double_names:\n",
    "                if double_names[word] in file[i-window:i+window] or \"leonean\" in file[i-window:i+window]:\n",
    "                    new_file.append(word + \"-\" + double_names[word])\n",
    "                    i += 1\n",
    "            elif word in triple_names:\n",
    "                new_file.append(word + \"-\" + triple_names[word][0] + \"-\" + triple_names[word][1])\n",
    "                if triple_names[word][0] in word_win:\n",
    "                    if triple_names[word][1] in word_win:\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        i += 1\n",
    "            elif word == \"alex\":\n",
    "                if 'brima' in word_win:\n",
    "                    new_file.append(\"alex-tamba-brima\")\n",
    "                    i += 2\n",
    "            elif word == \"tamba\":\n",
    "                new_file.append(\"alex-tamba-brima\")\n",
    "                i+=1\n",
    "            elif word == \"ibrahim\":\n",
    "                if 'bazzy' in word_win or 'kamara' in word_win:\n",
    "                    new_file.append(\"brima-kamara\")\n",
    "                    i += 2\n",
    "            elif word == \"sam\" or word == \"bockarie\":\n",
    "                if \"bockarie\" in word_win:\n",
    "                    new_file.append(\"mosquito\")\n",
    "                    i += 1\n",
    "            elif word == 'bazzy':\n",
    "                new_file.append(\"brima-kamara\")\n",
    "                i += 1\n",
    "            elif word == \"brima\":\n",
    "                if 'et' and 'al' in word_win:\n",
    "                    i += 2\n",
    "            else:\n",
    "                new_file.append(word)\n",
    "            i+=1\n",
    "        new_files.append(new_file)\n",
    "        i = 0\n",
    "    return new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_files(file_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_files = []\n",
    "    for file in file_list:\n",
    "        new_file = []\n",
    "        for word in file:\n",
    "            if word.lower() in SPECIAL_WORDS:\n",
    "                word = special_lemmatizer(word)\n",
    "            else:\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "            new_file.append(word)\n",
    "        lemmatized_files.append(new_file)\n",
    "    return lemmatized_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_lemmatizer(word):\n",
    "    word = word.lower()\n",
    "    if word.lower() == \"raped\":\n",
    "        return \"rape\"\n",
    "    if word.lower() == \"men\":\n",
    "        return \"man\"\n",
    "    if word == \"was\":\n",
    "        return \"is\"\n",
    "    if word == \"commander\":\n",
    "        return \"command\"\n",
    "    if word == \"worked\":\n",
    "        return \"work\"\n",
    "    if word == \"muslims\":\n",
    "        return \"muslim\"\n",
    "    if word == \"Serbs\":\n",
    "        return \"Serb\"\n",
    "    if word == \"men\":\n",
    "        return \"man\"\n",
    "    if word == \"gave\":\n",
    "        return \"give\"\n",
    "    if word == \"does\" or word == \"doent\":\n",
    "        return \"do\"\n",
    "    if word == \"happened\":\n",
    "        return \"happen\"\n",
    "    if word == \"vukovi\":\n",
    "        return \"vukovic\"\n",
    "    if word == \"herad\":\n",
    "        return \"hear\"\n",
    "    if word == \"fo\":\n",
    "        return \"foca\"\n",
    "    if word ==\"dragoljub\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(files_list, stop_words):\n",
    "    clean_files = []\n",
    "    for file in files_list:\n",
    "        c_file = []\n",
    "        for word in file:\n",
    "            if word.lower() not in stop_words and \"ÿ\" not in word and not word.isdigit():\n",
    "                c_file.append(word.lower())\n",
    "        clean_files.append(c_file)\n",
    "    return clean_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_freq(files_list):\n",
    "    frequencies = {}\n",
    "    for file in files_list:\n",
    "        for word in file:\n",
    "            if word not in frequencies:\n",
    "                frequencies[word] = 1\n",
    "            else:\n",
    "                frequencies[word] += 1\n",
    "    x = sorted((value, key) for (key, value) in frequencies.items())\n",
    "    x.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_words(file_list):\n",
    "    all = {}\n",
    "    for file in file_list:\n",
    "        for line in file:\n",
    "            line = line.split(\" \")\n",
    "            for word in line:\n",
    "                word = word.strip()\n",
    "                word = re.sub('[!#?.,-\\\\\\]', '', word)\n",
    "                word = word.title()\n",
    "                if str(word) not in all:\n",
    "                    all[str(word)] = 1\n",
    "                else:\n",
    "                    all[str(word)] += 1\n",
    "    all = sorted((value, key) for (key, value) in all.items())\n",
    "    all.reverse()\n",
    "    print(all)\n",
    "    print(len(all))\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(path):\n",
    "    file_list = get_files(path)\n",
    "    file_list = discard_enumeration(file_list)\n",
    "    file_list = tokenize_files(file_list)\n",
    "    file_list = basic_NER(file_list)\n",
    "    file_list = lemmatize_files(file_list)\n",
    "    file_list = clean(file_list, stop_words)\n",
    "    return file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lda_model(file_path, case=\"default\", num_topics=10, iterations=5000, save=True):\n",
    "    dictionary = corpora.Dictionary(file_path)\n",
    "    corpus = [dictionary.doc2bow(file) for file in file_path]\n",
    "    model = LdaMallet(PATH_TO_MALLET, corpus=corpus, num_topics=num_topics,\n",
    "                      id2word=dictionary, iterations=iterations)\n",
    "    now = datetime.datetime.now()\n",
    "    if save:\n",
    "        serial_name = name + \"_\" + str(num_topics) + \"_topics_\" + str(now.hour) + \"h\" + str(now.minute) + \"m\"\n",
    "        print(\"saving as \" + serial_name)\n",
    "        print(\" \")\n",
    "        model.save(serial_name)\n",
    "        save_topics_to_text(model, serial_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_topics_to_text(lda_model, name):\n",
    "    topics = lda_model.print_topics(num_words=20)\n",
    "    file = open(PATH_TO_TXT_MODELS + case + \"\\\\\" + name + \".txt\", 'w+')\n",
    "    for topic in topics:\n",
    "        file.write(\"Topic \" + str(topic[0]) + \": \\n\\n\")\n",
    "        words = topic[1].split(\"+\")\n",
    "        for i in range(len(words)):\n",
    "            word = words[i].split(\"*\")\n",
    "            word = word[1].strip()\n",
    "            word = word.replace(\"\\\"\", \"\")\n",
    "            file.write(word)\n",
    "            if i < len(words)-1:\n",
    "                file.write(\", \")\n",
    "            else:\n",
    "                file.write(\".\")\n",
    "            if i == 9:\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics_graph(dictionaries):\n",
    "    i = 1\n",
    "    for dict in dictionaries:\n",
    "        print(dict)\n",
    "        dict1 = [(v,k) for k,v in dict.items()]\n",
    "        dict1.sort(reverse=True)\n",
    "        x = [i for i in range(len(dict))]\n",
    "        y = [dict[str(i)] for i in range(len(dict))]\n",
    "        plot = plt.bar(x, y)\n",
    "        plt.title(\"subject frequency number: \" + str(i))\n",
    "        plt.xlim([0, len(dict)])\n",
    "        plt.show()\n",
    "        i += 1\n",
    "    labels = np.array(['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file(lda_model, file):\n",
    "    dct = Dictionary([file])  # fit dictionary\n",
    "    bow = dct.doc2bow(file)\n",
    "    doc_lda = lda_model[bow]\n",
    "    doc_lda.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(doc_lda)\n",
    "    return doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_dist(path, file_list, f_model, n_topics):\n",
    "    all_docs = []\n",
    "    file_index = 0\n",
    "    # 5 best topics\n",
    "    all_dicts = [dict() for i in range(5)]\n",
    "    for i in range(5):\n",
    "        for j in range(n_topics):\n",
    "            all_dicts[i][str(j)] = 0\n",
    "    file_names = [name for name in listdir(path) if isfile(join(path, name))]\n",
    "    for name, f in zip(file_names, file_list):\n",
    "        print(\"file number: \" + str(file_index) + \", file name: \" + name)\n",
    "        doc = analyze_file(f_model, f)\n",
    "        all_docs.append(doc)\n",
    "        for i in range(5):\n",
    "            all_dicts[i][str(doc[0][0])] += 1\n",
    "        # print(BORDER)\n",
    "        file_index += 1\n",
    "    print()\n",
    "    return all_dicts, all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_files(file_list, doc_name, processed_folders_path):\n",
    "    f = open(processed_folders_path + \"\\\\\" + doc_name + \".txt\", \"w\")\n",
    "    for file in file_list:\n",
    "        for word in file:\n",
    "            f.write(word)\n",
    "            f.write(\" \")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_from_file(doc_name, processed_folders_path):\n",
    "    file_list = []\n",
    "    f = open(processed_folders_path+ \"\\\\\" + doc_name + \".txt\")\n",
    "    for line in f.readlines():\n",
    "        file = [word for word in line.split(\" \")]\n",
    "        file_list.append(file)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_files(paths, saving_folder=PROCESSED_FILES_FOLDER):\n",
    "    for i in range(len(paths)):\n",
    "        p_files = create_data(paths[i])\n",
    "        save_processed_files(p_files, names[i], saving_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.05.16-CAR-OTP-PPPP-0925_MPED2.txt\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'C:\\Users\\thiya\\OneDrive\\NUS\\Year 3\\Sem 2\\RA\\Martin Buber\\Mallet\\bin\\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\thiya\\AppData\\Local\\Temp\\e6bb17_corpus.txt --output C:\\Users\\thiya\\AppData\\Local\\Temp\\e6bb17_corpus.mallet' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-089ee2601b7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"judgement\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"all_and_judgement\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"defense\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"prosecution\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"victims\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\thiya\\\\OneDrive\\\\NUS\\\\Year 3\\\\Sem 2\\\\RA\\\\Martin Buber\\\\Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_lda_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-fd1f7ec7205f>\u001b[0m in \u001b[0;36mcreate_lda_model\u001b[1;34m(file_path, case, num_topics, iterations, save)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     model = LdaMallet(PATH_TO_MALLET, corpus=corpus, num_topics=num_topics,\n\u001b[1;32m----> 5\u001b[1;33m                       id2word=dictionary, iterations=iterations)\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \"\"\"\n\u001b[1;32m--> 273\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;34m'--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[1;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpusmallet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"converting temporary corpus to MALLET format with %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m   1916\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command 'C:\\Users\\thiya\\OneDrive\\NUS\\Year 3\\Sem 2\\RA\\Martin Buber\\Mallet\\bin\\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\thiya\\AppData\\Local\\Temp\\e6bb17_corpus.txt --output C:\\Users\\thiya\\AppData\\Local\\Temp\\e6bb17_corpus.mallet' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    case = KUNARAC\n",
    "    name = \"\"\n",
    "    spec = \"all\"\n",
    "    dir_path = DATA_PATH + case + \"\\\\all\"\n",
    "    data_name = case + \"_\" + spec\n",
    "    n_topics, n_words, n_iterations = 3, 20, 10000\n",
    "    save = True\n",
    "    models = [\"all\", \"judgement\", \"all_and_judgement\", \"defense\", \"prosecution\", \"victims\"]\n",
    "    files = create_data(\"C:\\\\Users\\\\thiya\\\\OneDrive\\\\NUS\\\\Year 3\\\\Sem 2\\\\RA\\\\Martin Buber\\\\Data\")\n",
    "    model = create_lda_model(files, num_topics=n_topics, iterations=n_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.05.16-CAR-OTP-PPPP-0925_MPED2.txt\n"
     ]
    }
   ],
   "source": [
    "cleaned_data=create_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['questioned', 'by', 'm', 'bala', 'gaye', 'q', 'dr', 'reicherter', 'thank', 'you', 'again', 'for', 'your', 'presence', 'a', 'you', 're', 'welcome', 'q', 'i', 'just', 'wanted', 'to', 'touch', 'upon', 'something', 'that', 'mr', 'raised', 'in', 'relation', 'to', 'post', 'traumatic', 'stress', 'disorder', 'and', 'in', 'particular', 'page', 'to', 'of', 'your', 'report', 'and', 'if', 'i', 'can', 'just', 'read', 'out', 'the', 'extract', 'a', 'is', 'this', 'i', 'have', 'is', 'this', 'just', 'page', 'q', 'could', 'be', 'i', 'm', 'working', 'with', 'the', 'evidence', 'number', 'but', 'just', 'a', 'second', 'let', 'me', 'check', 'that', 'would', 'be', 'page', 'a', 'yeah', 'okay', 'q', 'to', 'and', 'one', 'of', 'the', 'thing', 'that', 'you', 'stated', 'is', 'the', 'change', 'in', 'the', 'brain', 'structure', 'and', 'function', 'that', 'are', 'associated', 'with', 'traumatic', 'stress', 'a', 'yes', 'q', 'exposure', 'can', 'result', 'in', 'cognitive', 'emotional', 'and', 'behavioural', 'difficulty', 'that', 'constitute', 'the', 'symptom', 'of', 'disorder', 'such', 'as', 'ptsd', 'depression', 'anxiety', 'dissociation', 'and', 'so', 'on', 'a', 'yes', 'q', 'now', 'i', 'know', 'we', 'focus', 'quite', 'a', 'bit', 'on', 'ptsd', 'mr', 'has', 'but', 'i', 'just', 'wanted', 'to', 'put', 'ptsd', 'aside', 'and', 'can', 'you', 'explain', 'how', 'these', 'other', 'grave', 'mental', 'outcome', 'can', 'affect', 'the', 'daily', 'functioning', 'of', 'survivor', 'and', 'victim', 'of', 'crime', 'such', 'as', 'rape', 'a', 'from', 'a', 'biological', 'standpoint', 'or', 'just', 'talking', 'about', 'their', 'functionality', 'in', 'general', 'q', 'the', 'way', 'the', 'biology', 'influence', 'their', 'actual', 'functioning', 'in', 'daily', 'life', 'a', 'yeah', 'i', 'mean', 'i', 'think', 'that', 's', 'one', 'of', 'the', 'thing', 'i', 'm', 'trying', 'to', 'raise', 'in', 'this', 'discussion', 'with', 'mr', 'and', 'now', 'in', 'answering', 'your', 'question', 'is', 'you', 'know', 'these', 'biological', 'change', 'take', 'place', 'and', 'you', 'know', 'subsequent', 'insult', 'can', 'make', 'them', 'worse', 'or', 'can', 'change', 'the', 'trajectory', 'of', 'the', 'outcome', 'right', 'and', 'some', 'of', 'the', 'mitigating', 'factor', 'that', 'i', 'had', 'mentioned', 'yesterday', 'like', 'mental', 'health', 'treatment', 'et', 'cetera', 'might', 'offset', 'these', 'or', 'make', 'these', 'better', 'in', 'the', 'situation', 'that', 's', 'being', 'presented', 'it', 'sound', 'like', 'these', 'change', 'occur', 'and', 'then', 'they', 'are', 'made', 'worse', 'and', 'worse', 'by', 'a', 'lack', 'of', 'mental', 'health', 'intervention', 'and', 'situation', 'getting', 'worse', 'and', 'worse', 'i', 'think', 'what', 'you', 'are', 'asking', 'is', 'what', 'other', 'sort', 'of', 'outcome', 'can', 'we', 'see', 'as', 'a', 'result', 'and', 'so', 'some', 'of', 'the', 'co', 'morbidity', 'in', 'addition', 'to', 'post', 'traumatic', 'stress', 'disorder', 'which', 'is', 'kind', 'of', 'the', 'most', 'commonly', 'spoken', 'about', 'one', 'would', 'be', 'other', 'mental', 'health', 'disorder', 'like', 'major', 'depressive', 'disorder', 'or', 'some', 'cognitive', 'dysfunction', 'so', 'specifically', 'what', 'you', 'are', 'referring', 'to', 'here', 'you', 're', 'talking', 'about', 'brain', 'structure', 'that', 'can', 'be', 'damaged', 'or', 'malfunction', 'as', 'a', 'result', 'usually', 'causing', 'cognitive', 'problem', 'like', 'difficulty', 'thinking', 'definitely', 'problem', 'with', 'memory', 'which', 'can', 'cause', 'day', 'to', 'day', 'problem', 'poor', 'judgment', 'simple', 'forgetfulness', 'the', 'classic', 'example', 'that', 's', 'anecdotal', 'from', 'my', 'experience', 'is', 'immigrant', 'who', 'are', 'who', 'come', 'to', 'the', 'united', 'state', 'and', 'trying', 'to', 'learn', 'english', 'and', 'are', 'having', 'a', 'very', 'very', 'difficult', 'time', 'doing', 'that', 'and', 'in', 'order', 'to', 'become', 'a', 'citizen', 'there', 'you', 'have', 'to', 'learn', 'english', 'it', 's', 'very', 'very', 'challenging', 'for', 'them', 'to', 'do', 'that', 'because', 'they', 'have', 'a', 'cognitive', 'problem', 'as', 'a', 'result', 'of', 'their', 'mental', 'health', 'disorder', 'and', 'so', 'they', 'need', 'special', 'accommodation', 'to', 'learn', 'a', 'new', 'language', 'i', 'mean', 'that', 'would', 'be', 'one', 'example', 'but', 'you', 'can', 'imagine', 'how', 'having', 'that', 'level', 'of', 'cognitive', 'dysfunction', 'could', 'create', 'all', 'sort', 'of', 'difficulty', 'in', 'day', 'to', 'day', 'life', 'q', 'because', 'what', 'i', 'am', 'trying', 'to', 'get', 'at', 'is', 'putting', 'aside', 'ptsd', 'which', 'requires', 'a', 'combination', 'of', 'a', 'number', 'of', 'mental', 'health', 'disorder', 'if', 'we', 'take', 'depression', 'on', 'it', 'own', 'for', 'instance', 'this', 'is', 'something', 'that', 'can', 'seriously', 'affect', 'someone', 's', 'life', 'for', 'instance', 'a', 'totally', 'totally', 'debilitating', 'yes', 'q', 'exactly', 'so', 'for', 'instance', 'you', 've', 'talked', 'about', 'it', 'impact', 'on', 'parenting', 'for', 'example', 'so', 'if', 'we', 'move', 'beyond', 'ptsd', 'just', 'to', 'make', 'sure', 'that', 'these', 'other', 'grave', 'outcome', 'can', 'have', 'just', 'as', 'serious', 'an', 'impact', 'on', 'the', 'victim', 'and', 'survivor', 'a', 'right', 'absolutely', 'so', 'i', 'hope', 'that', 'this', 'paper', 'this', 'report', 'has', 'detailed', 'post', 'traumatic', 'stress', 'disorder', 'but', 'i', 'am', 'also', 'quite', 'certain', 'that', 'it', 'has', 'talked', 'about', 'the', 'other', 'possible', 'outcome', 'of', 'traumatic', 'experience', 'including', 'mood', 'disorder', 'other', 'anxiety', 'disorder', 'cognitive', 'problem', 'et', 'cetera', 'q', 'and', 'i', 'think', 'you', 've', 'clarified', 'this', 'enough', 'but', 'i', 'just', 'want', 'to', 'make', 'sure', 'it', 's', 'clear', 'in', 'relation', 'to', 'for', 'instance', 'the', 'rape', 'that', 'were', 'committed', 'by', 'mlc', 'troop', 'your', 'testimony', 'if', 'i', 'understand', 'it', 'correctly', 'is', 'that', 'that', 'would', 'result', 'in', 'some', 'kind', 'of', 'traumatic', 'stress', 'disorder', 'or', 'other', 'grave', 'outcome', 'which', 'then', 'put', 'them', 'in', 'a', 'worse', 'off', 'situation', 'in', 'relation', 'to', 'the', 'current', 'event', 'in', 'the', 'central', 'african', 'republic', 'would', 'that', 'be', 'correct', 'a', 'yes', 'absolutely', 'so', 'i', 'think', 'mr', 'point', 'is', 'a', 'good', 'one', 'and', 'that', 'is', 'that', 'these', 'other', 'event', 'are', 'at', 'the', 'same', 'level', 'of', 'trauma', 'that', 'might', 'also', 'create', 'ptsd', 'in', 'survivor', 'but', 'you', 'know', 'what', 'i', 'm', 'trying', 'to', 'be', 'clear', 'about', 'is', 'that', 'the', 'fact', 'that', 'they', 've', 'already', 'been', 'exposed', 'to', 'traumatic', 'experience', 'the', 'fact', 'that', 'many', 'of', 'them', 'already', 'have', 'ptsd', 'make', 'them', 'far', 'more', 'vulnerable', 'to', 'any', 'stressor', 'that', 'come', 'along', 'a', 'minor', 'one', 'or', 'what', 'he', 's', 'pointing', 'out', 'a', 'major', 'one', 'right', 'so', 'the', 'outcome', 'are', 'predicted', 'to', 'be', 'far', 'worse', 'because', 'they', 'already', 'have', 'a', 'mental', 'health', 'outcome', 'from', 'the', 'first', 'experience', 'q', 'another', 'thing', 'you', 'touched', 'upon', 'is', 'the', 'issue', 'of', 'perception', 'of', 'safety', 'right', 'a', 'yes', 'q', 'from', 'a', 'victim', 'point', 'of', 'view', 'and', 'can', 'you', 'explain', 'the', 'way', 'in', 'which', 'even', 'after', 'objective', 'safety', 'is', 're', 'established', 'so', 'in', 'this', 'case', 'where', 'the', 'soldier', 'have', 'gone', 'back', 'to', 'the', 'democratic', 'republic', 'of', 'congo', 'how', 'the', 'victim', 'can', 'still', 'experience', 'certain', 'distress', 'or', 'danger', 'that', 'resurfaces', 'well', 'into', 'the', 'future', 'and', 'i', 'believe', 'in', 'particular', 'at', 'page', 'you', 'mention', 'that', 'the', 'feeling', 'of', 'danger', 'threat', 'and', 'helplessness', 'can', 'resurface', 'well', 'into', 'the', 'future', 'even', 'when', 'after', 'when', 'objective', 'safety', 'is', 're', 'established', 'can', 'you', 'just', 'explain', 'what', 'you', 'mean', 'by', 'that', 'a', 'yeah', 'i', 'can', 'explain', 'it', 'in', 'general', 'term', 'and', 'if', 'you', 'want', 'me', 'to', 'be', 'more', 'specific', 'please', 'you', 'know', 'one', 'of', 'the', 'attribute', 'of', 'ptsd', 'that', 'we', 'see', 'very', 'often', 'is', 'that', 'the', 'person', 'who', 'is', 'suffering', 'with', 'ptsd', 'sort', 'of', 'seems', 'to', 'be', 'in', 'a', 'perpetual', 'state', 'of', 'the', 'perception', 'of', 'non', 'safety', 'right', 'so', 'i', 've', 'you', 'know', 'spent', 'time', 'working', 'with', 'veteran', 'who', 'have', 'had', 'war', 'experience', 'now', 'they', 're', 'back', 'at', 'home', 'their', 'life', 'are', 'perfectly', 'safe', 'and', 'yet', 'they', 'stand', 'post', 'all', 'night', 'watching', 'the', 'door', 'to', 'make', 'sure', 'no', 'one', 'come', 'through', 'the', 'door', 'right', 'completely', 'illogical', 'and', 'they', 're', 'aware', 'of', 'it', 'they', 'have', 'the', 'perception', 'that', 'there', 'is', 'still', 'danger', 'and', 'that', 'they', 'are', 'still', 'in', 'danger', 'even', 'though', 'they', 'live', 'back', 'in', 'menlo', 'park', 'california', 'which', 'is', 'a', 'very', 'safe', 'community', 'some', 'of', 'them', 'actually', 'stand', 'vigil', 'with', 'a', 'gun', 'right', 'similar', 'with', 'rape', 'victim', 'very', 'often', 'you', 'know', 'sort', 'of', 'every', 'experience', 'with', 'a', 'male', 'will', 'be', 'perceived', 'as', 'something', 'of', 'a', 'very', 'dangerous', 'nature', 'they', 'cannot', 'be', 'involved', 'in', 'a', 'trusting', 'relationship', 'they', 're', 'always', 'under', 'the', 'perception', 'that', 'they', 'are', 'currently', 'unsafe', 'and', 'under', 'stress', 'q', 'thank', 'you', 'and', 'lastly', 'in', 'relation', 'to', 'the', 'mental', 'health', 'consequence', 'do', 'the', 'fact', 'that', 'rape', 'is', 'suffered', 'as', 'a', 'result', 'of', 'let', 's', 'say', 'in', 'the', 'context', 'of', 'genocide', 'rather', 'than', 'you', 'know', 'war', 'crime', 'or', 'crime', 'against', 'humanity', 'actually', 'affect', 'the', 'likelihood', 'of', 'the', 'grave', 'outcome', 'a', 'i', 'm', 'not', 'sure', 'i', 'understand', 'you', 'mean', 'the', 'purpose', 'of', 'the', 'rape', 'being', 'something', 'other', 'than', 'can', 'you', 'ask', 'the', 'question', 'again', 'please', 'q', 'let', 'me', 'clarify', 'so', 'one', 'of', 'the', 'when', 'you', 'talked', 'about', 'your', 'victim', 'from', 'cambodia', 'mr', 'made', 'the', 'point', 'that', 'that', 'is', 'in', 'relation', 'to', 'a', 'genocide', 'so', 'what', 'i', 'am', 'asking', 'is', 'whether', 'in', 'term', 'of', 'the', 'consequence', 'of', 'the', 'rape', 'would', 'a', 'rape', 'victim', 'who', 'suffered', 'rape', 'in', 'the', 'context', 'of', 'war', 'crime', 'be', 'different', 'from', 'genocide', 'or', 'from', 'your', 'mental', 'health', 'perspective', 'that', 'would', 'not', 'have', 'much', 'of', 'a', 'difference', 'a', 'they', 're', 'actually', 'q', 'that', 's', 'really', 'what', 'i', 'am', 'driving', 'it', 'a', 'sort', 'of', 'looking', 'at', 'that', 'literature', 'right', 'now', 'for', 'a', 'different', 'report', 'and', 'it', 's', 'not', 'clear', 'that', 'rape', 'in', 'the', 'context', 'of', 'genocide', 'is', 'an', 'extra', 'risk', 'factor', 'genocide', 'is', 'a', 'risk', 'factor', 'for', 'ptsd', 'and', 'has', 'specific', 'kind', 'of', 'mental', 'health', 'outcome', 'but', 'rape', 'for', 'the', 'purpose', 'of', 'genocide', 'or', 'for', 'the', 'purpose', 'of', 'this', 'notion', 'of', 'ethnic', 'cleansing', 'it', 's', 'not', 'clear', 'that', 'that', 'has', 'it', 'own', 'kind', 'of', 'mental', 'health', 'outcome', 'rape', 'is', 'a', 'risk', 'factor', 'for', 'ptsd', 'q', 'thank', 'you', 'very', 'much', 'm', 'bala', 'gaye', 'i', 've', 'concluded', 'my', 'question', 'thank', 'you', 'madam', 'president', 'presiding', 'judge', 'steiner', 'thank', 'you', 'very', 'much', 'm', 'bala', 'gaye', 'icc', 't', 'red', 'eng', 'wt', 'nb', 't', 'icc', 't', 'red', 'eng', 'wt', 'nb', 't', 'page', 'page', 'icc', 't', 'red', 'eng', 'wt', 'nb', 't', 'page']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'C:\\Users\\thiya\\OneDrive\\NUS\\Year 3\\Sem 2\\RA\\Martin Buber\\Mallet\\bin import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\thiya\\AppData\\Local\\Temp\\fd8f57_corpus.txt --output C:\\Users\\thiya\\AppData\\Local\\Temp\\fd8f57_corpus.mallet' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-d19fc934760a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_lda_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"default\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-fd1f7ec7205f>\u001b[0m in \u001b[0;36mcreate_lda_model\u001b[1;34m(file_path, case, num_topics, iterations, save)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     model = LdaMallet(PATH_TO_MALLET, corpus=corpus, num_topics=num_topics,\n\u001b[1;32m----> 5\u001b[1;33m                       id2word=dictionary, iterations=iterations)\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \"\"\"\n\u001b[1;32m--> 273\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;34m'--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[1;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpusmallet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"converting temporary corpus to MALLET format with %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m   1916\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command 'C:\\Users\\thiya\\OneDrive\\NUS\\Year 3\\Sem 2\\RA\\Martin Buber\\Mallet\\bin import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\thiya\\AppData\\Local\\Temp\\fd8f57_corpus.txt --output C:\\Users\\thiya\\AppData\\Local\\Temp\\fd8f57_corpus.mallet' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "create_lda_model(cleaned_data, case=\"default\", num_topics=10, iterations=5000, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
